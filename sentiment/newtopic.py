#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
topic_modeling.py

åŸºäº BERTweet å¥å‘é‡çš„ä¸»é¢˜å»ºæ¨¡ä¸èšç±»è¯„ä¼°ï¼š
1) æ¯”è¾ƒ KMeans / Agglomerative / HDBSCAN èšç±»è´¨é‡ï¼ˆSilhouette / DBI / CHï¼‰
2) ä½¿ç”¨ BERTopic è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼ˆåŸºäºé¢„è®¡ç®— embeddingï¼‰
3) ä¿å­˜ï¼š
   - clustering_eval.csv             å„èšç±»æ–¹æ³•æŒ‡æ ‡
   - topic_info.csv                  ä¸»é¢˜åˆ—è¡¨ä¸å…³é”®è¯
   - tweets_with_topics.csv          æ¯æ¡æ¨æ–‡çš„ä¸»é¢˜åˆ†é…
   - topic_trend_top10.png           æŒ‰å‘¨çš„å‰10ä¸ªä¸»é¢˜çƒ­åº¦æ›²çº¿
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

import hdbscan
from bertopic import BERTopic


# ==============================
# 1. è¯»å–æ•°æ®ä¸ embedding
# ==============================
TEXT_COL = "clean_text"
TIME_COL = "Datetime"

TWEET_FILE = "processed_tweets.csv"
EMB_FILE   = "tweet_embeddings_BERTWEET.npy"   # ä½ ä¹‹å‰ç”Ÿæˆçš„ embedding æ–‡ä»¶

print("ğŸ“‚ Loading tweets and embeddings...")
df = pd.read_csv(TWEET_FILE)
embeddings = np.load(EMB_FILE)

if TEXT_COL not in df.columns:
    raise ValueError(f"åˆ— {TEXT_COL} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ processed_tweets.csv")

if len(df) != embeddings.shape[0]:
    raise ValueError(f"æ ·æœ¬æ•°ä¸åŒ¹é…ï¼šdf={len(df)} vs embeddings={embeddings.shape[0]}")


# ==============================
# 2. èšç±»è¯„ä¼°ï¼šKMeans / Agglomerative / HDBSCAN
# ==============================
def evaluate_clustering_models(embeddings, max_samples=5000, random_state=42):
    """
    å¯¹åŒä¸€æ‰¹ embedding ä½¿ç”¨ä¸åŒèšç±»ç®—æ³•ï¼Œæ¯”è¾ƒï¼š
    - Silhouette Score
    - Davies-Bouldin Score
    - Calinski-Harabasz Score
    """
    n_samples = embeddings.shape[0]
    if n_samples > max_samples:
        idx = np.random.RandomState(random_state).choice(n_samples, max_samples, replace=False)
        X = embeddings[idx]
    else:
        X = embeddings

    results = []

    def safe_metrics(X, labels, name):
        """è®¡ç®—èšç±»æŒ‡æ ‡ï¼ˆå¤„ç† label å…¨ç›¸åŒã€å™ªå£°è¿‡å¤šçš„æƒ…å†µï¼‰"""
        labels = np.array(labels)
        # è¿‡æ»¤ HDBSCAN ä¸­çš„å™ªå£°ç‚¹
        mask = labels != -1
        if mask.sum() < 2 or len(np.unique(labels[mask])) < 2:
            print(f"âš  {name}: æœ‰æ•ˆèšç±»å¤ªå°‘ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ã€‚")
            return {
                "silhouette": np.nan,
                "davies_bouldin": np.nan,
                "calinski_harabasz": np.nan,
                "n_clusters": len(np.unique(labels[mask])),
                "noise_ratio": float((labels == -1).mean())
            }

        Xv = X[mask]
        lv = labels[mask]

        try:
            sil = silhouette_score(Xv, lv)
        except Exception:
            sil = np.nan
        try:
            dbi = davies_bouldin_score(Xv, lv)
        except Exception:
            dbi = np.nan
        try:
            ch = calinski_harabasz_score(Xv, lv)
        except Exception:
            ch = np.nan

        return {
            "silhouette": sil,
            "davies_bouldin": dbi,
            "calinski_harabasz": ch,
            "n_clusters": len(np.unique(lv)),
            "noise_ratio": float((labels == -1).mean())
        }

    # ---------- KMeans ----------
    for k in [20, 30, 40]:
        print(f"ğŸ”¹ Evaluating KMeans(k={k}) ...")
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = km.fit_predict(X)
        m = safe_metrics(X, labels, f"KMeans_k={k}")
        m["model"] = "KMeans"
        m["param"] = f"k={k}"
        results.append(m)

    # ---------- Agglomerative ----------
    for k in [20, 30]:
        print(f"ğŸ”¹ Evaluating Agglomerative(n_clusters={k}) ...")
        ag = AgglomerativeClustering(n_clusters=k)
        labels = ag.fit_predict(X)
        m = safe_metrics(X, labels, f"Agglomerative_k={k}")
        m["model"] = "Agglomerative"
        m["param"] = f"k={k}"
        results.append(m)

    # ---------- HDBSCAN ----------
    for min_cluster_size in [15, 30]:
        print(f"ğŸ”¹ Evaluating HDBSCAN(min_cluster_size={min_cluster_size}) ...")
        hdb = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=None,
            metric="euclidean",
            cluster_selection_method="eom"
        )
        labels = hdb.fit_predict(X)
        m = safe_metrics(X, labels, f"HDBSCAN_mcs={min_cluster_size}")
        m["model"] = "HDBSCAN"
        m["param"] = f"min_cluster_size={min_cluster_size}"
        results.append(m)

    eval_df = pd.DataFrame(results)
    return eval_df


print("\nğŸ“Š Evaluating clustering models (KMeans / Agglomerative / HDBSCAN)...")
eval_df = evaluate_clustering_models(embeddings)
eval_df.to_csv("clustering_eval.csv", index=False)
print("âœ” Saved: clustering_eval.csv")
print(eval_df)


# ==============================
# 3. ä½¿ç”¨ BERTopic è¿›è¡Œä¸»é¢˜å»ºæ¨¡
# ==============================
print("\nğŸš€ Fitting BERTopic with precomputed embeddings...")

docs = df[TEXT_COL].astype(str).tolist()

# è®© BERTopic ä½¿ç”¨æˆ‘ä»¬æä¾›çš„ embeddingï¼ˆä¸å†è‡ªå·±ç¼–ç ï¼‰
topic_model = BERTopic(
    embedding_model=None,   # ä¸ç”¨å†…ç½®çš„ embedding
    verbose=True,
)

topics, probs = topic_model.fit_transform(docs, embeddings=embeddings)

df["topic_id"] = topics
df["topic_prob"] = probs

# ä¸»é¢˜ä¿¡æ¯ï¼ˆIDã€å¤§å°ã€å…³é”®è¯ï¼‰
topic_info = topic_model.get_topic_info()
topic_info.to_csv("topic_info.csv", index=False, encoding="utf-8")
df.to_csv("tweets_with_topics.csv", index=False, encoding="utf-8")

print("âœ” Saved: topic_info.csv")
print("âœ” Saved: tweets_with_topics.csv")
print(f"å…±å¾—åˆ° {len(topic_info)} ä¸ª topicï¼ˆåŒ…å« -1 å™ªå£°ï¼‰")


# ==============================
# 4. å¯¹ BERTopic ç»“æœåšèšç±»æŒ‡æ ‡è¯„ä¼°
# ==============================
print("\nğŸ“ Evaluating BERTopic topic assignments...")

labels = np.array(topics)
mask = labels != -1

if mask.sum() > 1 and len(np.unique(labels[mask])) > 1:
    Xv = embeddings[mask]
    lv = labels[mask]

    try:
        sil_bertopic = silhouette_score(Xv, lv)
    except Exception:
        sil_bertopic = np.nan

    try:
        dbi_bertopic = davies_bouldin_score(Xv, lv)
    except Exception:
        dbi_bertopic = np.nan

    try:
        ch_bertopic = calinski_harabasz_score(Xv, lv)
    except Exception:
        ch_bertopic = np.nan
else:
    sil_bertopic = dbi_bertopic = ch_bertopic = np.nan

# å°è¯•è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§ï¼ˆcoherenceï¼‰
try:
    coherence = topic_model.get_coherence()
except Exception:
    coherence = np.nan

with open("bertopic_eval.txt", "w", encoding="utf-8") as f:
    f.write(f"Silhouette (BERTopic topics): {sil_bertopic}\n")
    f.write(f"Davies-Bouldin (BERTopic topics): {dbi_bertopic}\n")
    f.write(f"Calinski-Harabasz (BERTopic topics): {ch_bertopic}\n")
    f.write(f"Topic Coherence (BERTopic): {coherence}\n")

print("âœ” Saved: bertopic_eval.txt")
print("BERTopic clustering quality:")
print(f"  Silhouette       = {sil_bertopic}")
print(f"  Davies-Bouldin   = {dbi_bertopic}")
print(f"  Calinski-Harabasz= {ch_bertopic}")
print(f"  Coherence        = {coherence}")


# ==============================
# 5. ä¸»é¢˜æ—¶é—´è¶‹åŠ¿ï¼ˆæŒ‰å‘¨å‘å¸–é‡ï¼‰
# ==============================
print("\nğŸ“ˆ Computing topic trends over time...")

# è§£ææ—¶é—´
df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors="coerce")
df = df.dropna(subset=[TIME_COL])

df["week"] = df[TIME_COL].dt.to_period("W").dt.start_time

# å»æ‰å™ªå£° topic -1
df_valid = df[df["topic_id"] != -1].copy()

topic_counts = (
    df_valid
    .groupby(["week", "topic_id"])
    .size()
    .unstack(fill_value=0)
    .sort_index()
)

# é€‰å–æ•´ä½“ä¸Šæœ€çƒ­çš„å‰ 10 ä¸ª topic
total_counts = topic_counts.sum(axis=0).sort_values(ascending=False)
top_topics = total_counts.head(10).index.tolist()

topic_counts_top = topic_counts[top_topics]

plt.figure(figsize=(12, 6))
for t in top_topics:
    plt.plot(topic_counts_top.index, topic_counts_top[t], label=f"Topic {t}")

plt.xlabel("Week")
plt.ylabel("Tweet Count")
plt.title("Top 10 Topics - Weekly Volume")
plt.legend(loc="upper right", fontsize=8)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig("topic_trend_top10.png", dpi=300)
plt.close()

print("âœ” Saved: topic_trend_top10.png")
print("\nğŸ‰ All topic modeling steps completed!")
